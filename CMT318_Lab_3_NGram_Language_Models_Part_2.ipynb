{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itisEndymion/DARC/blob/main/CMT318_Lab_3_NGram_Language_Models_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CMT318: Lab 3 - NGram Language Models Part 2 (Autumn 2024)\n",
        "\n",
        "This notebook has been adapted from [UW's CSE 447 / CSE M 547 Project 1](https://drive.google.com/file/d/1Q1UD-vawgU-GfnvynWkV6nQD_3SizPho/view).\n",
        "\n",
        "**IMPORTANT: Save a copy of the notebook before working on it. You can also download it if you prefer**"
      ],
      "metadata": {
        "id": "VId-xGU2aDhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The data you need for this lab is available in Learning Central.\n",
        "\n",
        "You are provided with three data files (a subset of the One Billion Word Language Modeling Benchmark). Each line in each file contains a whitespace-tokenized sentence.\n",
        "\n",
        "- **1bbenchmark.train.tokens:** data for training your language models.\n",
        "- **1bbenchmark.dev.tokens:** data for debugging and choosing the best hyperparameters.\n",
        "- **1bbenchmark.test.tokens:** data for evaluating your language models.\n",
        "\n",
        "**IMPORTANT:**\n",
        "You will primarily use the development/validation dataset as the previously unseen data while (i) developing and testing your code, (ii) trying out different model and training design decisions, (iii) tuning the hyperparameters, and (iv) performing error analysis.\n",
        "For scientific integrity, it is extremely important that you **use the test data only once**, just before you report all the final results. Otherwise, you will start overfitting on the test set indirectly."
      ],
      "metadata": {
        "id": "-F_E-VE4bTYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 0: The `nltk.lm` package\n",
        "\n",
        "The Natural Language Toolkit includes a package specifically designed for n-gram language modeling: [nltk.lm](https://www.nltk.org/api/nltk.lm.html). Go through the tutorial included in the documentation to test some the functionalities offered by this package.\n",
        "\n",
        "**IMPORTANT:** If you wish, you are allowed to use functions from `nltk.lm` in the reminder of this lab."
      ],
      "metadata": {
        "id": "uWJZZpg3kldU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "\n"
      ],
      "metadata": {
        "id": "qqL4nrvGuhpK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Build Language Models\n",
        "You will build **unigram**, **bigram**, and **trigram** language models. To handle\n",
        "out-of-vocabulary (OOV) words, **convert tokens that occur less than three times in the training data into a special `<unk>` token during training**. Remember to also add the `<s>` and `</s>` tokens.\n",
        "\n",
        "You should have (at least) three functions called `unigram()`, `bigram()`, and `trigram()` that contain your implementations of these models, respectively. Feel free to add additional helper functions for better organization and/or debugging, but the above functions are required."
      ],
      "metadata": {
        "id": "7eWAnqAmcPLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def load_sentences(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip().split() for line in f.readlines()]\n",
        "\n",
        "\n",
        "def handle_oov(sentences, min_count=3):\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    vocab = {word for word, count in word_counts.items() if count >= min_count}\n",
        "\n",
        "    processed = [\n",
        "        [word if word in vocab else \"<unk>\" for word in sentence]\n",
        "        for sentence in sentences\n",
        "    ]\n",
        "    return processed, vocab\n",
        "\n",
        "\n",
        "def train_model(sentences, n):\n",
        "    train_data, vocab = padded_everygram_pipeline(n, sentences)\n",
        "    model = MLE(n)\n",
        "    model.fit(train_data, vocab)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZZgVu9xBdOCa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Evaluate Your Models with Perplexity\n",
        "Evaluate all your models using perplexity in two ways: (1) one without any smoothing technique; and (2) one with Laplace smoothing (also called add-one smoothing).\n",
        "\n",
        "Report the unsmoothed and smoothed with Laplace smoothing version of perplexity scores of the unigram, bigram, and trigram language models for your training, development, and test sets."
      ],
      "metadata": {
        "id": "gWIEs4Bcdfaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.lm import MLE, Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "def load_sentences(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip().split() for line in f.readlines()]\n",
        "\n",
        "def handle_oov(sentences, min_count=3):\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    vocab = {word for word, count in word_counts.items() if count >= min_count}\n",
        "    processed = [\n",
        "        [word if word in vocab else \"<unk>\" for word in sentence]\n",
        "        for sentence in sentences\n",
        "    ]\n",
        "    return processed, vocab\n",
        "\n",
        "def train_mle_model(sentences, n):\n",
        "    train_data, vocab = padded_everygram_pipeline(n, sentences)\n",
        "    model = MLE(n)\n",
        "    model.fit(train_data, vocab)\n",
        "    return model\n",
        "\n",
        "def train_laplace_model(sentences, n):\n",
        "    train_data, vocab = padded_everygram_pipeline(n, sentences)\n",
        "    model = Laplace(n)\n",
        "    model.fit(train_data, vocab)\n",
        "    return model\n",
        "\n",
        "def evaluate_perplexity(model, data_sentences, n):\n",
        "    test_ngrams = [list(ngrams(pad_both_ends(sentence, n), n)) for sentence in data_sentences]\n",
        "    all_ngrams = [ng for sent in test_ngrams for ng in sent]\n",
        "    return model.perplexity(all_ngrams)\n",
        "\n",
        "def evaluate_all(train_path, dev_path, test_path):\n",
        "    n_values = [1, 2, 3]\n",
        "    models = {'MLE': train_mle_model, 'Laplace': train_laplace_model}\n",
        "    datasets = {\n",
        "        'train': load_sentences(train_path),\n",
        "        'dev': load_sentences(dev_path),\n",
        "        'test': load_sentences(test_path)\n",
        "    }\n",
        "\n",
        "    train_sentences_raw = datasets['train']\n",
        "    train_sentences, vocab = handle_oov(train_sentences_raw)\n",
        "\n",
        "    datasets = {\n",
        "        name: [[word if word in vocab else \"<unk>\" for word in sentence] for sentence in sents]\n",
        "        for name, sents in datasets.items()\n",
        "    }\n",
        "\n",
        "    for n in n_values:\n",
        "        print(f\"\\n{n}-gram Models \")\n",
        "        for model_name, train_fn in models.items():\n",
        "            model = train_fn(train_sentences, n)\n",
        "            for set_name in ['train', 'dev', 'test']:\n",
        "                ppl = evaluate_perplexity(model, datasets[set_name], n)\n",
        "                print(f\"{model_name} {n}-gram - {set_name} perplexity: {ppl:.2f}\")\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_all(\n",
        "    train_path=\"1b_benchmark.train.tokens\",\n",
        "    dev_path=\"1b_benchmark.dev.tokens\",\n",
        "    test_path=\"1b_benchmark.test.tokens\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "EUNswwD3eHUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2405506-f79a-4547-ca2c-3770a4229164"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1-gram Models \n",
            "MLE 1-gram - train perplexity: 1083.22\n",
            "MLE 1-gram - dev perplexity: 985.98\n",
            "MLE 1-gram - test perplexity: 991.53\n",
            "Laplace 1-gram - train perplexity: 1084.32\n",
            "Laplace 1-gram - dev perplexity: 988.44\n",
            "Laplace 1-gram - test perplexity: 993.89\n",
            "\n",
            "2-gram Models \n",
            "MLE 2-gram - train perplexity: 77.07\n",
            "MLE 2-gram - dev perplexity: inf\n",
            "MLE 2-gram - test perplexity: inf\n",
            "Laplace 2-gram - train perplexity: 1442.39\n",
            "Laplace 2-gram - dev perplexity: 1669.75\n",
            "Laplace 2-gram - test perplexity: 1665.48\n",
            "\n",
            "3-gram Models \n",
            "MLE 3-gram - train perplexity: 7.30\n",
            "MLE 3-gram - dev perplexity: inf\n",
            "MLE 3-gram - test perplexity: inf\n",
            "Laplace 3-gram - train perplexity: 4633.13\n",
            "Laplace 3-gram - dev perplexity: 7071.58\n",
            "Laplace 3-gram - test perplexity: 7040.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Interpolation\n",
        "To make your language model work better, you will implement **linear interpolation** smoothing between unigram, bigram, and trigram models. You should use the development data to choose the best values for the hyperparameters: $\\lambda_1, \\lambda_2, \\lambda_3$, for unigram, bigram and trigram, respectively. For this lab, you should try a few combinations to find reasonable values, but remember that $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$\n",
        "\n",
        "We ask you to:\n",
        "\n",
        "1. Experiment and report perplexity scores on training and development sets for five sets of values of $\\lambda_1, \\lambda_2, \\lambda_3$ that you\n",
        "tried, along with short descriptions of the strategies that you used to find better hyperparameters.\n",
        "2. Report the training and development perplexity for the values $\\lambda_1 = 0.1, \\lambda_2 = 0.3, \\lambda_3 = 0.6$.\n",
        "3. Report perplexity on the test set, using the best combination of\n",
        "hyperparameters that you chose from the development set. Specify those hyperparameters."
      ],
      "metadata": {
        "id": "nQ3byYAQeRnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def load_sentences(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip().split() for line in f.readlines()]\n",
        "\n",
        "def handle_oov(sentences, min_count=3):\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    vocab = {word for word, count in word_counts.items() if count >= min_count}\n",
        "    processed = [\n",
        "        [word if word in vocab else \"<unk>\" for word in sentence]\n",
        "        for sentence in sentences\n",
        "    ]\n",
        "    return processed, vocab\n",
        "\n",
        "def train_mle_model(sentences, n):\n",
        "    data, vocab = padded_everygram_pipeline(n, sentences)\n",
        "    model = MLE(n)\n",
        "    model.fit(data, vocab)\n",
        "    return model\n",
        "\n",
        "def linear_interpolated_score(word, context, models, lambdas):\n",
        "    n = len(context) + 1\n",
        "    total = 0.0\n",
        "    for i, lm in enumerate(models):\n",
        "        if i + 1 > n:\n",
        "            continue\n",
        "        sub_context = context[-(i):] if i > 0 else []\n",
        "        total += lambdas[i] * lm.score(word, sub_context)\n",
        "    return total\n",
        "\n",
        "def evaluate_interpolated_perplexity(models, data_sentences, lambdas, n=3):\n",
        "    total_log_prob = 0.0\n",
        "    total_words = 0\n",
        "    epsilon = 1e-10  # To prevent log(0)\n",
        "\n",
        "    for sentence in data_sentences:\n",
        "        sentence = list(pad_both_ends(sentence, n))\n",
        "        for i in range(n - 1, len(sentence)):\n",
        "            context = sentence[i - (n - 1):i]\n",
        "            word = sentence[i]\n",
        "            prob = linear_interpolated_score(word, context, models, lambdas)\n",
        "            prob = max(prob, epsilon)  # Replace zero with small positive value\n",
        "            total_log_prob += -math.log(prob, 2)\n",
        "            total_words += 1\n",
        "\n",
        "    return 2 ** (total_log_prob / total_words)\n",
        "\n",
        "\n",
        "train_raw = load_sentences(\"1b_benchmark.train.tokens\")\n",
        "dev_raw = load_sentences(\"1b_benchmark.dev.tokens\")\n",
        "test_raw = load_sentences(\"1b_benchmark.test.tokens\")\n",
        "\n",
        "train_data, vocab = handle_oov(train_raw)\n",
        "dev_data = [[w if w in vocab else \"<unk>\" for w in s] for s in dev_raw]\n",
        "test_data = [[w if w in vocab else \"<unk>\" for w in s] for s in test_raw]\n",
        "\n",
        "# Train n-gram models\n",
        "uni = train_mle_model(train_data, 1)\n",
        "bi = train_mle_model(train_data, 2)\n",
        "tri = train_mle_model(train_data, 3)\n",
        "\n",
        "models = [uni, bi, tri]\n",
        "lambdas = [0.1, 0.3, 0.6]\n",
        "\n",
        "# Evaluate\n",
        "print(\"Train perplexity:\", evaluate_interpolated_perplexity(models, train_data, lambdas))\n",
        "print(\"Dev perplexity:\", evaluate_interpolated_perplexity(models, dev_data, lambdas))"
      ],
      "metadata": {
        "id": "NcY8wicjgMsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05794a17-8e69-4f54-8a97-65b10dd0fdb3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train perplexity: 10.401568218017589\n",
            "Dev perplexity: 287.1731994415753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Analysis\n",
        "\n",
        "Provide answers to the following questions supporting them with appropriate empirical experimental evidence:\n",
        "\n",
        "1. If you use half of the training data, would it increase or decrease the perplexity of previously unseen data? Why?\n",
        "2. If you convert all tokens that appeared less than five times to `<unk>` (a special symbol for out-of-vocabulary tokens), would it increase or decrease the perplexity on the previously unseen data compared to an approach that converts only those words that appeared just once to `<unk>`? Why?"
      ],
      "metadata": {
        "id": "4pLeFg9TgQQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.It would increase the perplexity on unseen data. Because reducing the amount of training data increases perplexity, due to poorer n-gram coverage.\n"
      ],
      "metadata": {
        "id": "3opRNzFw2oKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "def load_sentences(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip().split() for line in f]\n",
        "\n",
        "def handle_oov(sentences, min_count=3):\n",
        "    word_counts = Counter(word for sent in sentences for word in sent)\n",
        "    vocab = {w for w, c in word_counts.items() if c >= min_count}\n",
        "    processed = [[w if w in vocab else \"<unk>\" for w in sent] for sent in sentences]\n",
        "    return processed, vocab\n",
        "\n",
        "def train_laplace_model(sentences, n):\n",
        "    train_data, vocab = padded_everygram_pipeline(n, sentences)\n",
        "    model = Laplace(n)\n",
        "    model.fit(train_data, vocab)\n",
        "    return model\n",
        "\n",
        "def evaluate_perplexity(model, data_sentences, n):\n",
        "    test_ngrams = [list(ngrams(pad_both_ends(sent, n), n)) for sent in data_sentences]\n",
        "    all_ngrams = [ng for sent in test_ngrams for ng in sent]\n",
        "    return model.perplexity(all_ngrams)\n",
        "\n",
        "def question1_half_training(train_path, dev_path):\n",
        "    full_raw = load_sentences(train_path)\n",
        "    half_raw = full_raw[: len(full_raw) // 2]\n",
        "    dev_raw = load_sentences(dev_path)\n",
        "\n",
        "    full_proc, vocab = handle_oov(full_raw)\n",
        "    half_proc = [[w if w in vocab else \"<unk>\" for w in s] for s in half_raw]\n",
        "    dev_proc = [[w if w in vocab else \"<unk>\" for w in s] for s in dev_raw]\n",
        "\n",
        "    full_model = train_laplace_model(full_proc, 3)\n",
        "    half_model = train_laplace_model(half_proc, 3)\n",
        "\n",
        "    ppl_full = evaluate_perplexity(full_model, dev_proc, 3)\n",
        "    ppl_half = evaluate_perplexity(half_model, dev_proc, 3)\n",
        "\n",
        "    print(f\"Dev perplexity using FULL training data: {ppl_full:.2f}\")\n",
        "    print(f\"Dev perplexity using HALF training data: {ppl_half:.2f}\")\n",
        "\n",
        "\n",
        "question1_half_training(\"1b_benchmark.train.tokens\", \"1b_benchmark.dev.tokens\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiDvUKZv3tWH",
        "outputId": "2303c766-4ce1-4f78-f85d-14b8779b721c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev perplexity using FULL training data: 7071.58\n",
            "Dev perplexity using HALF training data: 8361.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2..It would decrease the perplexity on unseen data. Because mapping more rare words to `<unk>` improves generalization and reduces perplexity, as the model learns more useful fallback behavior."
      ],
      "metadata": {
        "id": "Ynbvqlrt3nT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def question2_unk_threshold(train_path, dev_path, threshold_1=2, threshold_2=5):\n",
        "    train_raw = load_sentences(train_path)\n",
        "    dev_raw = load_sentences(dev_path)\n",
        "\n",
        "    data1, vocab1 = handle_oov(train_raw, min_count=threshold_1)\n",
        "    dev1 = [[w if w in vocab1 else \"<unk>\" for w in s] for s in dev_raw]\n",
        "    model1 = train_laplace_model(data1, 3)\n",
        "    ppl1 = evaluate_perplexity(model1, dev1, 3)\n",
        "\n",
        "    data2, vocab2 = handle_oov(train_raw, min_count=threshold_2)\n",
        "    dev2 = [[w if w in vocab2 else \"<unk>\" for w in s] for s in dev_raw]\n",
        "    model2 = train_laplace_model(data2, 3)\n",
        "    ppl2 = evaluate_perplexity(model2, dev2, 3)\n",
        "\n",
        "    print(f\"Dev perplexity with <unk> threshold {threshold_1}: {ppl1:.2f}\")\n",
        "    print(f\"Dev perplexity with <unk> threshold {threshold_2}: {ppl2:.2f}\")\n",
        "\n",
        "# Example usage\n",
        "question2_unk_threshold(\"1b_benchmark.train.tokens\", \"1b_benchmark.dev.tokens\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-rVvRU83vhu",
        "outputId": "28797091-a1de-43e2-f563-fd72165a2c89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev perplexity with <unk> threshold 2: 10340.83\n",
            "Dev perplexity with <unk> threshold 5: 4450.16\n"
          ]
        }
      ]
    }
  ]
}